{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YMkYa0Gd9OX",
        "outputId": "8e0d6c25-6063-435c-8078-637d844b55dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# IMPORT LIBRARIES\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "\n",
        "# Check if GPU is available and set the device accordingly\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Bigram Model"
      ],
      "metadata": {
        "id": "gyu1fKwLCW3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "gnru12CerL2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with a dataset, we will use sp500_earnings_transcripts which contains thousands of real corporate earnings call.\n",
        "\n",
        "# 1. Install the Hugging Face datasets library\n",
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "print(\"Downloading full S&P 500 earnings transcripts...\")\n",
        "\n",
        "# 2. Load the proper text-based dataset\n",
        "dataset = load_dataset(\"Bose345/sp500_earnings_transcripts\", split=\"train\")\n",
        "\n",
        "# 3. Extract the 'content' column (the full raw transcript text)\n",
        "# Pulling 45 full transcripts for a sizeable dataset\n",
        "transcripts = dataset['content'][:45]\n",
        "text = \"\\n\\n--- NEXT EARNINGS CALL ---\\n\\n\".join(transcripts)\n",
        "\n",
        "# 4. Save it locally as input.txt\n",
        "with open('input.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(text)\n",
        "\n",
        "file_size_mb = os.path.getsize('input.txt') / (1024 * 1024)\n",
        "print(f\"File saved. Size: {file_size_mb:.2f} MB\")\n",
        "print(f\"Length of dataset in characters: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtEXXjlvfz8t",
        "outputId": "9cd5bcb4-8f94-47e7-f4c2-1f221d414525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n",
            "Downloading full S&P 500 earnings transcripts...\n",
            "File saved. Size: 2.30 MB\n",
            "Length of dataset in characters: 2404755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtJi4cqBmYgW",
        "outputId": "f8585643-47a9-4152-9022-77a82250cbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operator: Good afternoon, and welcome to the Agilent Technologies Fourth Quarter Earnings Conference Call. All lines have been placed on mute to prevent any background noise. After the speakers' remarks, there will be a question-and-answer session. [Operator Instructions] Thank you. And now, I'd like to introduce you to the host for today's conference, Ankur Dhingra, Vice President of Investor Relations. Sir, please go ahead.\n",
            "Ankur Dhingra: Thank you, and welcome everyone to Agilent's fourth quarter and full-year conference call for fiscal year 2020. With me are Mike McMullen, Agilent's President and CEO; and Bob McMahon, Agilent's Senior Vice President and CFO. Joining in the Q&A after Bob's comments will be: Jacob Thaysen, President of Agilent's Life Sciences & Applied Markets Group; Sam Raha, President of Agilent's Diagnostics and Genomics Group; and Padraig McDonnell, President of Agilent CrossLab Group. This presentation is being webcast live. The news release, Investor presentati\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:**\n",
        " Keeping speaker labels like \"Operator\" and \"Ankur Dhingra\" teaches the model the structural flow of a corporate dialogue, making text generation more realistic and contextually aware. A --- NEXT EARNINGS CALL --- string separates each call and acts as a end of text token so the model knows the previous conversation has ended and a brand new topic is beginning."
      ],
      "metadata": {
        "id": "EAbZ8ThXmqOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "Kwp9tdDzrSVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get all unique characters that occur in this text (The Vocabulary)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Vocabulary: {''.join(chars)}\")\n",
        "\n",
        "# 2. Create the Tokenizer mapping\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: string -> list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: list of ints -> string\n",
        "\n",
        "# 3. Test the tokenizer\n",
        "test_string = \"Q3 revenue increased\"\n",
        "encoded_str = encode(test_string)\n",
        "print(f\"\\nEncoded '{test_string}': {encoded_str}\")\n",
        "print(f\"Decoded back: {decode(encoded_str)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFA-YAb5lXwQ",
        "outputId": "ab8de24c-00bf-4577-d860-12b2fc11606d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 94\n",
            "Vocabulary: \n",
            " !\"#$%&'()+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyzÉàè–—‘’“”…⁠\n",
            "\n",
            "Encoded 'Q3 revenue increased': [45, 19, 1, 74, 61, 78, 61, 70, 77, 61, 1, 65, 70, 59, 74, 61, 57, 75, 61, 60]\n",
            "Decoded back: Q3 revenue increased\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization simply converts raw text as a string to some sequence of integers according to some vocabulary of possible elements. Here we have 94 possible elements (including spaces). Each unique element is converted to a unique integer."
      ],
      "metadata": {
        "id": "8Muu8On6ooDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We take all the text, enocde it and wrap it into a torch.tensor, giving us the data tensor\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0daOEGG-pdtU",
        "outputId": "abdf6cad-00ef-4fde-9c6c-0f7c1551c2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2404755]) torch.int64\n",
            "tensor([43, 72, 61, 74, 57, 76, 71, 74, 26,  1, 35, 71, 71, 60,  1, 57, 62, 76,\n",
            "        61, 74, 70, 71, 71, 70, 12,  1, 57, 70, 60,  1, 79, 61, 68, 59, 71, 69,\n",
            "        61,  1, 76, 71,  1, 76, 64, 61,  1, 29, 63, 65, 68, 61, 70, 76,  1, 48,\n",
            "        61, 59, 64, 70, 71, 68, 71, 63, 65, 61, 75,  1, 34, 71, 77, 74, 76, 64,\n",
            "         1, 45, 77, 57, 74, 76, 61, 74,  1, 33, 57, 74, 70, 65, 70, 63, 75,  1,\n",
            "        31, 71, 70, 62, 61, 74, 61, 70, 59, 61,  1, 31, 57, 68, 68, 14,  1, 29,\n",
            "        68, 68,  1, 68, 65, 70, 61, 75,  1, 64, 57, 78, 61,  1, 58, 61, 61, 70,\n",
            "         1, 72, 68, 57, 59, 61, 60,  1, 71, 70,  1, 69, 77, 76, 61,  1, 76, 71,\n",
            "         1, 72, 74, 61, 78, 61, 70, 76,  1, 57, 70, 81,  1, 58, 57, 59, 67, 63,\n",
            "        74, 71, 77, 70, 60,  1, 70, 71, 65, 75, 61, 14,  1, 29, 62, 76, 61, 74,\n",
            "         1, 76, 64, 61,  1, 75, 72, 61, 57, 67, 61, 74, 75,  8,  1, 74, 61, 69,\n",
            "        57, 74, 67, 75, 12,  1, 76, 64, 61, 74, 61,  1, 79, 65, 68, 68,  1, 58,\n",
            "        61,  1, 57,  1, 73, 77, 61, 75, 76, 65, 71, 70, 13, 57, 70, 60, 13, 57,\n",
            "        70, 75, 79, 61, 74,  1, 75, 61, 75, 75, 65, 71, 70, 14,  1, 55, 43, 72,\n",
            "        61, 74, 57, 76, 71, 74,  1, 37, 70, 75, 76, 74, 77, 59, 76, 65, 71, 70,\n",
            "        75, 56,  1, 48, 64, 57, 70, 67,  1, 81, 71, 77, 14,  1, 29, 70, 60,  1,\n",
            "        70, 71, 79, 12,  1, 37,  8, 60,  1, 68, 65, 67, 61,  1, 76, 71,  1, 65,\n",
            "        70, 76, 74, 71, 60, 77, 59, 61,  1, 81, 71, 77,  1, 76, 71,  1, 76, 64,\n",
            "        61,  1, 64, 71, 75, 76,  1, 62, 71, 74,  1, 76, 71, 60, 57, 81,  8, 75,\n",
            "         1, 59, 71, 70, 62, 61, 74, 61, 70, 59, 61, 12,  1, 29, 70, 67, 77, 74,\n",
            "         1, 32, 64, 65, 70, 63, 74, 57, 12,  1, 50, 65, 59, 61,  1, 44, 74, 61,\n",
            "        75, 65, 60, 61, 70, 76,  1, 71, 62,  1, 37, 70, 78, 61, 75, 76, 71, 74,\n",
            "         1, 46, 61, 68, 57, 76, 65, 71, 70, 75, 14,  1, 47, 65, 74, 12,  1, 72,\n",
            "        68, 61, 57, 75, 61,  1, 63, 71,  1, 57, 64, 61, 57, 60, 14,  0, 29, 70,\n",
            "        67, 77, 74,  1, 32, 64, 65, 70, 63, 74, 57, 26,  1, 48, 64, 57, 70, 67,\n",
            "         1, 81, 71, 77, 12,  1, 57, 70, 60,  1, 79, 61, 68, 59, 71, 69, 61,  1,\n",
            "        61, 78, 61, 74, 81, 71, 70, 61,  1, 76, 71,  1, 29, 63, 65, 68, 61, 70,\n",
            "        76,  8, 75,  1, 62, 71, 77, 74, 76, 64,  1, 73, 77, 57, 74, 76, 61, 74,\n",
            "         1, 57, 70, 60,  1, 62, 77, 68, 68, 13, 81, 61, 57, 74,  1, 59, 71, 70,\n",
            "        62, 61, 74, 61, 70, 59, 61,  1, 59, 57, 68, 68,  1, 62, 71, 74,  1, 62,\n",
            "        65, 75, 59, 57, 68,  1, 81, 61, 57, 74,  1, 18, 16, 18, 16, 14,  1, 51,\n",
            "        65, 76, 64,  1, 69, 61,  1, 57, 74, 61,  1, 41, 65, 67, 61,  1, 41, 59,\n",
            "        41, 77, 68, 68, 61, 70, 12,  1, 29, 63, 65, 68, 61, 70, 76,  8, 75,  1,\n",
            "        44, 74, 61, 75, 65, 60, 61, 70, 76,  1, 57, 70, 60,  1, 31, 33, 43, 27,\n",
            "         1, 57, 70, 60,  1, 30, 71, 58,  1, 41, 59, 41, 57, 64, 71, 70, 12,  1,\n",
            "        29, 63, 65, 68, 61, 70, 76,  8, 75,  1, 47, 61, 70, 65, 71, 74,  1, 50,\n",
            "        65, 59, 61,  1, 44, 74, 61, 75, 65, 60, 61, 70, 76,  1, 57, 70, 60,  1,\n",
            "        31, 34, 43, 14,  1, 38, 71, 65, 70, 65, 70, 63,  1, 65, 70,  1, 76, 64,\n",
            "        61,  1, 45,  7, 29,  1, 57, 62, 76, 61, 74,  1, 30, 71, 58,  8, 75,  1,\n",
            "        59, 71, 69, 69, 61, 70, 76, 75,  1, 79, 65, 68, 68,  1, 58, 61, 26,  1,\n",
            "        38, 57, 59, 71, 58,  1, 48, 64, 57, 81, 75, 61, 70, 12,  1, 44, 74, 61,\n",
            "        75, 65, 60, 61, 70, 76,  1, 71, 62,  1, 29, 63, 65, 68, 61, 70, 76,  8,\n",
            "        75,  1, 40, 65, 62, 61,  1, 47, 59, 65, 61, 70, 59, 61, 75,  1,  7,  1,\n",
            "        29, 72, 72, 68, 65, 61, 60,  1, 41, 57, 74, 67, 61, 76, 75,  1, 35, 74,\n",
            "        71, 77, 72, 27,  1, 47, 57, 69,  1, 46, 57, 64, 57, 12,  1, 44, 74, 61,\n",
            "        75, 65, 60, 61, 70, 76,  1, 71, 62,  1, 29, 63, 65, 68, 61, 70, 76,  8,\n",
            "        75,  1, 32, 65, 57, 63, 70, 71, 75, 76, 65, 59, 75,  1, 57, 70, 60,  1,\n",
            "        35, 61, 70, 71, 69, 65, 59, 75,  1, 35, 74, 71, 77, 72, 27,  1, 57, 70,\n",
            "        60,  1, 44, 57, 60, 74, 57, 65, 63,  1, 41, 59, 32, 71, 70, 70, 61, 68,\n",
            "        68, 12,  1, 44, 74, 61, 75, 65, 60, 61, 70, 76,  1, 71, 62,  1, 29, 63,\n",
            "        65, 68, 61, 70, 76,  1, 31, 74, 71, 75, 75, 40, 57, 58,  1, 35, 74, 71,\n",
            "        77, 72, 14,  1, 48, 64, 65, 75,  1, 72, 74, 61, 75, 61, 70, 76, 57, 76,\n",
            "        65, 71, 70,  1, 65, 75,  1, 58, 61, 65, 70, 63,  1, 79, 61, 58, 59, 57,\n",
            "        75, 76,  1, 68, 65, 78, 61, 14,  1, 48, 64, 61,  1, 70, 61, 79, 75,  1,\n",
            "        74, 61, 68, 61, 57, 75, 61, 12,  1, 37, 70, 78, 61, 75, 76, 71, 74,  1,\n",
            "        72, 74, 61, 75, 61, 70, 76, 57, 76, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/val split"
      ],
      "metadata": {
        "id": "mXCDt4_erap1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"Training data length: {len(train_data):,} tokens\")\n",
        "print(f\"Validation data length: {len(val_data):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw6n7Tlxquhy",
        "outputId": "0def3511-723c-4f86-ca7b-ee2e34b312a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 2,164,279 tokens\n",
            "Validation data length: 240,476 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader\n",
        "\n",
        "We don't feed the Trasnsformer the entire dataset all at once as its computationally expensive. We sample and train small chunks at a time, defined by the block size or context window. We can train on context between 1 all the wat to block size. For example if block size is 8, that chunk will have 8 different predictions to train on.\n",
        "\n",
        "Iteratively, after we reach the end of the block size, the Transformer starts truncating to accomodate its most recent prediction to be part of the new context. It never receives more than block size input when predicting the next character.\n",
        "\n",
        "Everytime we feed a chunk of text into a Transformer, we have many batches of multiple chunks of text that are stacked up in a single tensor, for efficiency and parallelization.\n",
        "\n",
        "How many independent sequences will we process in parallel?\n",
        "Input size = [batch size, context window]"
      ],
      "metadata": {
        "id": "17TMTT_vrj0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range (block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"When input is {context}, the target is {target}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06E7yWctrimx",
        "outputId": "13c5bb78-579c-4a9f-87d7-9c40fd2f622f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([43]), the target is 72.\n",
            "When input is tensor([43, 72]), the target is 61.\n",
            "When input is tensor([43, 72, 61]), the target is 74.\n",
            "When input is tensor([43, 72, 61, 74]), the target is 57.\n",
            "When input is tensor([43, 72, 61, 74, 57]), the target is 76.\n",
            "When input is tensor([43, 72, 61, 74, 57, 76]), the target is 71.\n",
            "When input is tensor([43, 72, 61, 74, 57, 76, 71]), the target is 74.\n",
            "When input is tensor([43, 72, 61, 74, 57, 76, 71, 74]), the target is 26.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4 # How many independent sequences will we process in parallel?\n",
        "block_size = 8 # What is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # Generate a small batch of data for inputs X and target Y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 4 random indexes to scoop out data from\n",
        "    x = torch.stack([data[i:i+block_size]for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('---')\n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f'When input is {context.tolist()}, the target is {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDI864ENu6tV",
        "outputId": "555f8073-4a43-4a14-ca67-525639d26954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[76,  1, 81, 71, 77,  1, 69, 57],\n",
            "        [76, 64, 61, 74, 57, 72, 81,  1],\n",
            "        [71,  1, 79, 64, 65, 59, 64,  1],\n",
            "        [ 1, 71, 70,  1, 76, 64, 61, 75]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 81, 71, 77,  1, 69, 57, 81],\n",
            "        [64, 61, 74, 57, 72, 81,  1, 60],\n",
            "        [ 1, 79, 64, 65, 59, 64,  1, 65],\n",
            "        [71, 70,  1, 76, 64, 61, 75, 61]])\n",
            "---\n",
            "When input is [76], the target is 1\n",
            "When input is [76, 1], the target is 81\n",
            "When input is [76, 1, 81], the target is 71\n",
            "When input is [76, 1, 81, 71], the target is 77\n",
            "When input is [76, 1, 81, 71, 77], the target is 1\n",
            "When input is [76, 1, 81, 71, 77, 1], the target is 69\n",
            "When input is [76, 1, 81, 71, 77, 1, 69], the target is 57\n",
            "When input is [76, 1, 81, 71, 77, 1, 69, 57], the target is 81\n",
            "When input is [76], the target is 64\n",
            "When input is [76, 64], the target is 61\n",
            "When input is [76, 64, 61], the target is 74\n",
            "When input is [76, 64, 61, 74], the target is 57\n",
            "When input is [76, 64, 61, 74, 57], the target is 72\n",
            "When input is [76, 64, 61, 74, 57, 72], the target is 81\n",
            "When input is [76, 64, 61, 74, 57, 72, 81], the target is 1\n",
            "When input is [76, 64, 61, 74, 57, 72, 81, 1], the target is 60\n",
            "When input is [71], the target is 1\n",
            "When input is [71, 1], the target is 79\n",
            "When input is [71, 1, 79], the target is 64\n",
            "When input is [71, 1, 79, 64], the target is 65\n",
            "When input is [71, 1, 79, 64, 65], the target is 59\n",
            "When input is [71, 1, 79, 64, 65, 59], the target is 64\n",
            "When input is [71, 1, 79, 64, 65, 59, 64], the target is 1\n",
            "When input is [71, 1, 79, 64, 65, 59, 64, 1], the target is 65\n",
            "When input is [1], the target is 71\n",
            "When input is [1, 71], the target is 70\n",
            "When input is [1, 71, 70], the target is 1\n",
            "When input is [1, 71, 70, 1], the target is 76\n",
            "When input is [1, 71, 70, 1, 76], the target is 64\n",
            "When input is [1, 71, 70, 1, 76, 64], the target is 61\n",
            "When input is [1, 71, 70, 1, 76, 64, 61], the target is 75\n",
            "When input is [1, 71, 70, 1, 76, 64, 61, 75], the target is 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Language Model, loss and generation\n",
        "\n",
        "A Bigram Language Model creates a token embedding table of (vocab_size, vocab_size). Every single integer is going to refer to the embedding table and pluck out a row of that table coressponding to its index. PyTorch arranges that into (B,T,C) which is interpreted as logits which is basically the scores for the next character in the sequence.\n",
        "\n",
        "B = batch, T = context window, C = vocab_size\n",
        "\n",
        "We use -log likelihood loss on predictions and target. We are expecting the loss to be -ln(1/94) = ~ 4.543 in which the model gives every possible character rougly equal probabilities, essentially just guessing."
      ],
      "metadata": {
        "id": "444wvVf-xqoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Each token directly reads off the logits from the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B*T, C)\n",
        "          targets = targets.view(B*T)\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # Focus only on last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1jGm0Yexu_A",
        "outputId": "857494de-976d-4518-e89a-6839d855119d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 94])\n",
            "tensor(5.0279, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens = 100)[0].tolist())) # Printing garbage as model is not trained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-21fQC14joP",
        "outputId": "6608a8ab-ca19-4964-c6a7-4566be243b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "R“w((%O”— ;nHMrIg⁠Bk–k–9&YNnsL0)-P\n",
            "O3IdYG)?N8,0'JEs–kBohR&\"Oè2[–Éspr’E1nJ‘oY3al\"+‘Lcy1-P+$zGgK1LsLSà\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Bigram Model"
      ],
      "metadata": {
        "id": "81k98gwy7X-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW8aJTEV7dwo",
        "outputId": "b5cdaa4d-190c-40ed-9824-005900983e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4587090015411377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens = 600)[0].tolist())) # Printing something a bit more reasonable as our Bigram Model trains and decreases its loss to 2.46"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wCIzn3L8gTF",
        "outputId": "cb24cb10-f81f-409d-d433-b9fe23d8505e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ok f w iount nke trtharonicMcowevoreto ARins stomole lels Ch wahex\n",
            "Micon f cacr jusk bureavee bu ou'sengoull r asorenuce tind idengma maus, a, s e Mas on ld. t, I e benecck: thand Ourithisem, ghinvexcor wer thel. inigu lonnad- t cerulll fonacor ictt wt d t ts f s ande.97. f dariMconever aid We t qut omer.7-Phare Bustminthe harowecey ad asua recy, aren negicincoug cank d l, thandun towiow. age s nuikepu les ithempathron llincty, big An W.\n",
            "RUn imathe bomof tha g ase thedido ifecs We tour wexce is, cade. a aredoures bllon'sowind s. aly Aneron shingro t bld tlongon s wnok iout abu tesevioberore ac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention"
      ],
      "metadata": {
        "id": "HTyvJtEE_XnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A token should only talk to previous tokens and not future tokens because you are trying to predict the future. We were able to use batch matrix multiply to perform a weighted aggregation in which the weights are specified by a T,T array with torch.tril.\n",
        "\n",
        "You can convert all 0's to be negative infinity in the tril matrix so that when softmax is applied, they become 0 whereas the lower side of the triangle averages out."
      ],
      "metadata": {
        "id": "RVWvgX9aAs8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# Single head of attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyyXPR4I_Vgg",
        "outputId": "685d482d-ad9e-4f0a-8e1f-5e73ee741502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The need for Self Attention**\n",
        "\n",
        "We don't want previous tokens to be all uniform. Different tokens should find other tokens more or less interesting and we want it to be data dependent. A vowel should be interested in what previous consonants are and for that information to flow to that vowel.\n",
        "\n",
        "Self attention solves this by emitting three vectors for each token, a query, key and value.\n",
        "\n",
        "**Query** = What am I looking for?\n",
        "\n",
        "**Key** = What do I contain?\n",
        "\n",
        "A dot product is computed as a measure of 'alignment', looking for keys which aligns well or 'answer' certain queries. This means a token places a higher emphasis on other tokens that have keys well aligned with their own queries.\n",
        "\n",
        "**Attention Weights** =  The dot product of Q and K, calculates the \"How much\" is passed.\n",
        "\n",
        "**Value** = If you find me relevant, here is how much I will actually tell you.\n",
        "\n",
        "When we do aggregation, we don't aggregate the tokens exactly. We aggregate a vector V instead of the raw value."
      ],
      "metadata": {
        "id": "qZtyRh9xL6L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHKvyYOWA5GF",
        "outputId": "d90ad030-8811-491e-9cb3-28dbf570f4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0865, 0.9135, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.6873, 0.3053, 0.0074, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0083, 0.1928, 0.6549, 0.1440, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0893, 0.3930, 0.4423, 0.0285, 0.0469, 0.0000, 0.0000, 0.0000],\n",
              "        [0.8100, 0.0827, 0.0333, 0.0159, 0.0483, 0.0098, 0.0000, 0.0000],\n",
              "        [0.0395, 0.0994, 0.0386, 0.5205, 0.2477, 0.0471, 0.0072, 0.0000],\n",
              "        [0.0095, 0.1311, 0.2246, 0.1646, 0.0944, 0.2340, 0.0520, 0.0898]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The need for Feed-Forward Neural Networks**\n",
        "\n",
        "Tokens looked at each other but didn't really have time to think on what they've found from the other tokens. A FeedFoward layer is added after the self-attend, it works on the per token level. The self attention is the communication, once they gathered all the data, they need to think on that data individually.\n",
        "\n",
        "**The need for Residual Connections**\n",
        "\n",
        "Starting to get a deep NN means it can suffer from optimzation issues. Here is where we need skip conncections (residual). In skip connections, computations happens from top to bottom (residual pathway), you are free to fork off from the pathway, perform some computation and then project back to the residual pathway via addition.\n",
        "During backpropogation, addition distributes gradients equally to both of its branches that fed as the input. The gradients from the loss hop through every addition note all the way to the input, also fork off through the residual blocks. We have this gradient superhighway from the supervision all the way to the input unimpeded. Residual blocks are initilized in the beginning to contribute very little to the residual pathway. In the beginning they are basically not there but as the optimization occurs, they come online over time which dramatically helps with the optimization.\n",
        "\n",
        "**The need for Layer Normalization**\n",
        "\n",
        "Batch Normalization made sure any individual neuron have unit gaussian distribution (0 mean, 1 s.d.). In LayerNorm you dont normalise the columns, you normalize the rows."
      ],
      "metadata": {
        "id": "Obqvsrq4Aeez"
      }
    }
  ]
}